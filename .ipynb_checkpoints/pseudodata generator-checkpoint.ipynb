{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development notes\n",
    "\n",
    "Define _messiness_ as variations in data types or completeness.\n",
    "Define _noise_ as outliers.\n",
    "\n",
    "1. Create noise arguments in add_arrays:\n",
    "    * Research best method. Probably overlaying (by addition or multiplication) another random distribution.\n",
    "    * What argument to accept? Float between 0 and 1, where 0 is no noise, and 1 makes the array completely random?\n",
    "2. Create messiness:\n",
    "    * Apply messiness arguments to each feature after generating the dataframe.\n",
    "    * Let user specify percent of data that would be NaN.\n",
    "    * Let user specify percent of data that's incorrect type. (e.g., value = str(numerical_value); float_value = int(float_value))\n",
    "    * Let user specify percent of outliers.\n",
    "3. Downcasting in _make_arrays:\n",
    "    * Allow arrays to have other numerical data types. What if the distributions yield integers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently working on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging**\n",
    "* Add_noise\n",
    "\n",
    "\n",
    "**Runtime optimization**\n",
    "* Analyze operation run times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#DEBUGGING\n",
    "from pprint import pprint\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Pseudodata:\n",
    "    \"\"\"\n",
    "    Anticipated workflow\n",
    "    1. Create Pseudodata instance.\n",
    "    2. Add two or more distribution arrays.\n",
    "    3. Generate an output pseudodata DataFrame from the distribution arrays.\n",
    "    4. Add an additional array to the dataset. Give option to update the pseudodata DataFrame automatically or manually (with method).\n",
    "    5. Output the pseudodata DataFrame as Pandas object. OPTIONAL OTHER FORMATS?\n",
    "    6. VISUALIZE THE DATAFRAME?\n",
    "    \n",
    "    Development plan\n",
    "    * Add 'messiness' arguments\n",
    "    * Allow non-numerical data arrays; i.e., text. (MAP STRING VALUES TO NUMBERS, THEN SUBSTITUTE BACK AFTER MAKING DATAFRAME?)\n",
    "    \n",
    "    Ideas to consider\n",
    "    * \"dataframe\" objects are tuples of the DataFrames and snapshots of data_profiles used for generation. I.e., dataframe = (data_profile, pandas.DataFrame).\n",
    "    * Distribution arrays should be stored. Advantage: able to reference values later and reconstruct DataFrame objects. Disadvantage: memory storage.\n",
    "    * DataFrames are best visualized with pairplots, especially for multiple (2+) dimensions.\n",
    "    * Users should have a method for exporting DataFrames as CSVs and SQL files, even if it just calls pandas.DataFrame.to_csv()\n",
    "    \"\"\"\n",
    "    reduced_memory = bool\n",
    "    data_profile = dict()\n",
    "    features_data = dict()\n",
    "    dataframe = pandas.DataFrame()\n",
    "    \n",
    "    def __init__(self, conserve_memory=False):\n",
    "        \"\"\"Downcast datatypes to reduce memory allocations.\"\"\"\n",
    "        self.reduced_memory = conserve_memory\n",
    "        self.data_profile = dict()\n",
    "        self.features_data = dict()\n",
    "        self.dataframe = pandas.DataFrame()\n",
    "        self._check_setup()\n",
    "    \n",
    "    def _check_setup(self):\n",
    "        try:\n",
    "            modules\n",
    "        except:\n",
    "            from sys import modules\n",
    "        for module in ['pandas', 'numpy']:\n",
    "            if module not in modules:\n",
    "                print(\"{} not imported\".format(module))\n",
    "\n",
    "    def get_data_profile(self):\n",
    "        \"\"\"Displays a description of the arrays in the Pseudodata instance as a DataFrame object.\"\"\"\n",
    "        return pandas.DataFrame(self.data_profile).T\n",
    "\n",
    "    def list_available_distributions(self, detailed_list=False):\n",
    "        \"\"\"Returns a list of the univariate distributions available in numpy.random.\"\"\"\n",
    "        rand_docstring = numpy.random.__doc__\n",
    "        prefiltered_doc_string = rand_docstring.split('variate distributions')[1].split('\\n')\n",
    "        dist_filter = filter(lambda x: 'distribution' in x, prefiltered_doc_string)\n",
    "        dist_list = [element.split(' ')[0] for element in dist_filter]\n",
    "        if detailed_list == False:\n",
    "            return dist_list\n",
    "        else:\n",
    "            detailed_dist_list = list()\n",
    "            for distribution in dist_list:\n",
    "                dist_docstring = eval(\"numpy.random.{}.__doc__\".format(distribution))\n",
    "                details = dist_docstring.split('\\n')[1].strip()\n",
    "                detailed_dist_list.append(details)\n",
    "            return detailed_dist_list    \n",
    "\n",
    "    def _is_evaluatable(self, input_string):\n",
    "        \"\"\"Used in _format_function_string(). Tests if a string refers to an object that can be evaluated.\"\"\"\n",
    "        try:\n",
    "            eval(input_string)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _is_callable(self, input_string):\n",
    "        \"\"\"Used in _format_function_string(). Tests if a string refers to a callable object.\"\"\"\n",
    "        try:\n",
    "            return callable(eval(input_string))\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _add_size(self, input_string, size):\n",
    "        \"\"\"Used in _format_function_string(). Adds a size argument to distribution function string if applicable.\"\"\"\n",
    "        if 'size' in input_string or size == None:\n",
    "            return input_string\n",
    "        elif '()' in input_string:\n",
    "            return input_string.replace('()', '(size={0})'.format(size))\n",
    "        else:\n",
    "            return input_string.replace(')', ', size={0})'.format(size))\n",
    "\n",
    "    def _format_function_string(self, input_string, size_argument=None):\n",
    "        \"\"\"Used in _make_data_array. Makes any necessary changes to the distribution function string so that it can make feature data.\"\"\"\n",
    "        if input_string.split('(')[0] in self.list_available_distributions():\n",
    "            input_string = 'numpy.random.{0}'.format(input_string)\n",
    "        if self._is_evaluatable(input_string) == False:\n",
    "            return input_string\n",
    "        callable_string_cases = {True: input_string + \"()\", \n",
    "                                 False: input_string}\n",
    "        callable_string = callable_string_cases.get(self._is_callable(input_string))\n",
    "        test_string = self._add_size(callable_string, size_argument)\n",
    "        try:\n",
    "            eval(test_string)\n",
    "            return test_string\n",
    "        except:\n",
    "            return callable_string\n",
    "    \n",
    "    def _make_data_array(self, distribution='normal', list_len=10):\n",
    "        \"\"\"Used in _add_feature(). Creates a 1-D Numpy array of specified length using the specified univariate distribution function from Numpy.random.\"\"\"\n",
    "        function_string = self._format_function_string(input_string=distribution, size_argument=list_len)\n",
    "        test_value = eval(function_string)\n",
    "        if isinstance(test_value, type(numpy.array([]))) or isinstance(test_value, list):\n",
    "            result = eval(function_string)\n",
    "        else:\n",
    "            result = [eval(function_string) for _ in range(list_len)]\n",
    "        result_array = numpy.array(result)\n",
    "        if self.reduced_memory == True:\n",
    "            result_array = result_array.astype('float32', casting='same_kind')\n",
    "        return result_array\n",
    "\n",
    "    def _make_Nd_dataframe(self, arrays, feature_suffix='', time_estimate=False, verbose_processing=False):\n",
    "        \"\"\"Used in _generate_dataframe(). Handles N-dimensions.\"\"\"\n",
    "        def verbose_message(string, end='\\n'):\n",
    "            if verbose_processing == True:\n",
    "                print(f\"{string}\", end=end)\n",
    "        n_points = numpy.product([array.shape[0] for array in arrays])\n",
    "        verbose_message(\"Generating DataFrame for {0:,} points, which could take about {1:.0f} seconds.\".format(n_points, n_points * 1.080935554128843 * 10 ** -6))\n",
    "        verbose_message(\"Setting up data grid.\", end=\" \")\n",
    "        grid = numpy.meshgrid(*arrays)\n",
    "        verbose_message(\"Reshaping data grid.\", end=\" \")\n",
    "        transposed_grid = numpy.transpose(grid)\n",
    "        raveled_grid = transposed_grid.ravel()\n",
    "        reshaped_grid = raveled_grid.reshape(-1, len(arrays))\n",
    "        verbose_message(\"Calculating products.\", end=\" \")\n",
    "        product_array = numpy.product(reshaped_grid, axis=1)\n",
    "        verbose_message(\"Creating sorted DataFrame.\", end=\" \")\n",
    "        sorted_index = product_array.argsort()\n",
    "        dataframe = pandas.DataFrame(reshaped_grid[sorted_index, :], index=product_array[sorted_index])\n",
    "        for column in dataframe:\n",
    "            feature_name = \"{0}{1}\".format(feature_suffix, column)\n",
    "            self.data_profile[column]['feature_name'] = feature_name\n",
    "        dataframe.columns = [\"{0}{1}\".format(feature_suffix, column) for column in dataframe.columns]            \n",
    "        verbose_message(\"Done.\", end=\" \")\n",
    "        return dataframe\n",
    "    \n",
    "    def add_feature(self, distribution='normal', size=10, noise_fraction=0.0, remake_dataframe=False):\n",
    "        \"\"\"Adds a 1-D data array to the Pseudodata instance of a specified length with noise applied to a fraction ofpoints. (Note: noise is applied when generating the DataFrame.) \n",
    "        \n",
    "        In addition to user-created distribution functions, refer to Pseudodata.list_available_distributions() to see available distibution options. \n",
    "        \n",
    "        Examples:\n",
    "        Pseudodata_instance.add_array()\n",
    "        Pseudodata_instance.add_array(distribution='poisson')\n",
    "        Pseudodata_instance.add_array(distribution='binomial(10, .5, size=10)')\n",
    "        Pseudodata_instance.add_array(distribution='logistic(loc=5.0, scale=2.0)', size=10)\n",
    "        \"\"\"\n",
    "        data_array = self._make_data_array(distribution=distribution, list_len=size)\n",
    "        feature_id = max(self.data_profile.keys(), default=-1) + 1\n",
    "        self.data_profile[feature_id] = {'feature_name': feature_id,\n",
    "                                         'size': data_array.shape[0], \n",
    "                                         'distribution': distribution, \n",
    "                                         'noise_fraction': noise_fraction}\n",
    "        self.features_data[feature_id] = data_array\n",
    "        if remake_dataframe == True:\n",
    "            self._make_Nd_dataframe(self.features_data.values())\n",
    "\n",
    "    def add_noise(self, feature_index=None):\n",
    "        \"\"\"Adds simple noise in-place by randomly translating a fraction of points.\"\"\"\n",
    "        if feature_index != None:\n",
    "            feature_noise_dict = {self.data_profile[feature]['feature_name']:self.data_profile[feature]['noise_fraction'] for feature in feature_index}\n",
    "        else:\n",
    "            feature_noise_dict = {feature['feature_name']: feature['noise_fraction'] for feature in self.data_profile.values()}\n",
    "        for feature_name in feature_noise_dict:\n",
    "            noise_fraction = feature_noise_dict[feature_name]\n",
    "            sample_index = self.dataframe.loc[:, feature_name].sample(frac=noise_fraction).index\n",
    "            size = sample_index.shape[0]\n",
    "            min_max_tuple = (self.dataframe.loc[:, feature_name].min(), self.dataframe.loc[:, feature_name].max())\n",
    "            generator = (numpy.random.uniform(*min_max_tuple) for _ in range(size))\n",
    "            noise_translation_array = numpy.fromiter(generator, numpy.float32, count=size)\n",
    "            self.dataframe.loc[sample_index, feature_name] = noise_translation_array\n",
    "    \n",
    "    def remove_feature(self, feature_index, remake_dataframe=False):\n",
    "        \"\"\"Removes a feature by data profile index.\"\"\"\n",
    "        self.data_profile.pop(feature_index, None)\n",
    "        self.features_data.pop(feature_index, None)\n",
    "        if remake_dataframe == True:\n",
    "            self._make_Nd_dataframe(self.features_data.values())\n",
    "            \n",
    "#     def remove_all_features(self, clear_dataframe=False):\n",
    "#         \"\"\"Deletes all data features, with an option to remove the DataFrame.\"\"\"\n",
    "#         self.data_profile = dict()\n",
    "#         self.features_data = dict()\n",
    "#         if clear_dataframe == True:\n",
    "#             self.dataframe = pandas.DataFrame()\n",
    "            \n",
    "    def generate_dataframe(self, feature_index_list=None, feature_name_suffix='feature_', reduce_memory=False, with_time_estimate=False, verbose=False):\n",
    "        \"\"\"Generates a new dataframe from all features in the Pseudodata instance or from a specified list of data profile indicies.\n",
    "        There is an option to reduce memory size of the dataframe by downcasting, which can significantly cut memory by reducing data precision.\"\"\"\n",
    "        def verbose_message(string, end='\\n'):\n",
    "            if verbose == True:\n",
    "                print(string, end=end)\n",
    "        if feature_index_list != None:\n",
    "            for item in feature_index_list:\n",
    "                if item not in self.data_profile:\n",
    "                    raise KeyError(\"{0} is not in the data profile.\".format(item))\n",
    "            verbose_message(\"Collecting data.\")\n",
    "            filtered_features_data = [self.features_data.get(key) for key in feature_index]\n",
    "            feature_array = numpy.array(filtered_features_data)\n",
    "            verbose_message(\"Making DataFrame\")\n",
    "            self.dataframe = self._make_Nd_dataframe(feature_array, feature_suffix=feature_name_suffix, time_estimate=with_time_estimate)\n",
    "            verbose_message(\"\\nAdding feature noise.\")\n",
    "            self.add_noise(feature_index)\n",
    "        else:\n",
    "            verbose_message(\"Making DataFrame\")\n",
    "            self.dataframe = self._make_Nd_dataframe(self.features_data.values(), feature_suffix=feature_name_suffix, time_estimate=with_time_estimate, verbose_processing=verbose)\n",
    "            verbose_message(\"\\nAdding feature noise.\")\n",
    "            self.add_noise()\n",
    "        verbose_message(\"Generation complete. The [{0},{1}] DataFrame can be reviewed with Pseudodata.get_dataframe().\".format(*self.dataframe.shape))\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Returns the pandas.DataFrame for the Pseudodata instance.\"\"\"\n",
    "        if self.dataframe.shape[0] != 0:\n",
    "            return self.dataframe\n",
    "        \n",
    "    def get_feature_data(self, feature_index=None):\n",
    "        \"\"\"Returns data for specific feature according to its data profile index.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def export_dataframe(self, filename = 'pseudodata_export', filepath='.', extension='csv'):\n",
    "        self.dataframe.to_csv(\"{filepath}/{filename}.csv\")\n",
    "        \n",
    "    def regenerate_feature(self, feature_index):\n",
    "        \"\"\"Resamples a specific feature according to its data profile index.\"\"\"\n",
    "        feature_details = self.data_profile.get(feature_index)\n",
    "        data_array = self._make_data_array(feature_details['size'], feature_details['distribution'])\n",
    "        self.features_data[feature_index] = data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging add_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pseudodata = Pseudodata(conserve_memory=True)\n",
    "for _ in range(4):\n",
    "    test_pseudodata.add_feature(size=13, noise_fraction=0.1)\n",
    "\n",
    "# test_pseudodata.generate_dataframe(verbose=True)\n",
    "# \"\"\"ValueError: shape mismatch: value array of shape (2856,) could not be broadcast to indexing result of shape (2857,)\"\"\"\n",
    "\n",
    "dataframe = test_pseudodata._make_Nd_dataframe(test_pseudodata.features_data.values())\n",
    "data_profile = test_pseudodata.data_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (2856,) could not be broadcast to indexing result of shape (2860,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2dbad7f39874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pseudodata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6931b5cb3772>\u001b[0m in \u001b[0;36madd_noise\u001b[0;34m(self, feature_index)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmin_max_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mnoise_translation_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_translation_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremake_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[0;32m--> 651\u001b[0;31m                                                     value=value)\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3693\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'setitem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value, mgr)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# coerce and try to infer the dtypes of the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (2856,) could not be broadcast to indexing result of shape (2860,)"
     ]
    }
   ],
   "source": [
    "test_pseudodata.add_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0', '1', '2', '3'], dtype='object')\n",
      "{'0': 0.1, '1': 0.1, '2': 0.1, '3': 0.1}\n",
      "Size: 2856\n",
      "Noise array shape: (2856,)\n",
      "Sample index shape: (2856,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (2856,) could not be broadcast to indexing result of shape (2857,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ed09ec92f2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-ed09ec92f2e4>\u001b[0m in \u001b[0;36madd_noise\u001b[0;34m(data_profile, dataframe, feature_index)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Noise array shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_translation_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample index shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_translation_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtemp_dataframe\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[0;32m--> 651\u001b[0;31m                                                     value=value)\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3693\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'setitem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value, mgr)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# coerce and try to infer the dtypes of the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (2856,) could not be broadcast to indexing result of shape (2857,)"
     ]
    }
   ],
   "source": [
    "# def add_noise(self, feature_index=None):\n",
    "def add_noise(data_profile, dataframe, feature_index=None):\n",
    "    dataframe_backup = dataframe.copy()\n",
    "    if feature_index != None:\n",
    "        feature_noise_dict = {data_profile[feature]['feature_name']: data_profile[feature]['noise_fraction'] for feature in feature_index}\n",
    "    else:\n",
    "        feature_noise_dict = {feature['feature_name']: feature['noise_fraction'] for feature in data_profile.values()}\n",
    "    print(feature_noise_dict)\n",
    "    for feature_name in feature_noise_dict:\n",
    "        noise_fraction = feature_noise_dict[feature_name]\n",
    "        sample_index = dataframe.loc[:, feature_name].sample(frac=noise_fraction).index\n",
    "        size = sample_index.shape[0]\n",
    "        print(\"Size:\", size)\n",
    "        min_max_tuple = (dataframe.loc[:, feature_name].min(), dataframe.loc[:, feature_name].max())\n",
    "        generator = (numpy.random.uniform(*min_max_tuple) for _ in range(size))\n",
    "        noise_translation_array = numpy.fromiter(generator, numpy.float32, count=size)\n",
    "        print(\"Noise array shape:\", noise_translation_array.shape)\n",
    "        print(\"Sample index shape:\", sample_index.shape)\n",
    "        dataframe.loc[sample_index, feature_name] = noise_translation_array\n",
    "    \n",
    "    temp_dataframe  = dataframe\n",
    "    dataframe = dataframe_backup\n",
    "    return temp_dataframe\n",
    "\n",
    "print(dataframe.columns)\n",
    "add_noise(data_profile, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfunc(a):\n",
    "    a = 1\n",
    "    a_backup = a\n",
    "    a = a * 0\n",
    "    temp_a = a\n",
    "    a = a_backup\n",
    "    return a, temp_a\n",
    "myfunc(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance time testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_pseudodata = Pseudodata(conserve_memory=False)\n",
    "\n",
    "# for _ in range(4):\n",
    "#     test_pseudodata.add_feature(size=70)\n",
    "\n",
    "# def timed_make_Nd_dataframe(pseudodata_obj):\n",
    "#     \"\"\"Handles N-dimensions.\"\"\"\n",
    "#     time_log = []\n",
    "#     start_time = time()\n",
    "    \n",
    "#     arrays = pseudodata_obj.features_data.values()\n",
    "#     time_log.append(('Init array', time()))\n",
    "                    \n",
    "#     grid = numpy.meshgrid(*arrays)\n",
    "#     time_log.append(('Make meshgrid', time()))\n",
    "    \n",
    "#     transposed_grid = numpy.transpose(grid)\n",
    "#     time_log.append(('Transpose grids', time()))\n",
    "    \n",
    "#     raveled_grid = transposed_grid.ravel()\n",
    "#     time_log.append(('Ravel grid', time()))\n",
    "    \n",
    "#     reshaped_grid = raveled_grid.reshape(-1, len(arrays))\n",
    "#     time_log.append(('Reshape grid', time()))\n",
    "\n",
    "#     product_array = numpy.product(reshaped_grid, axis=1)\n",
    "#     time_log.append(('Make product array', time()))\n",
    "\n",
    "#     sorted_index = product_array.argsort()\n",
    "#     time_log.append(('Sort product array', time()))\n",
    "\n",
    "#     dataframe = pandas.DataFrame(reshaped_grid[sorted_index, :], index=product_array[sorted_index])\n",
    "#     time_log.append(('Convert array to DataFrame', time()))\n",
    "\n",
    "#     dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "#     time_log.append(('Rename columns', time()))\n",
    "#     return dataframe, time_log\n",
    "\n",
    "    \n",
    "# def process_time_log(time_log):\n",
    "#     print(\"Runtime: {0:.6f} seconds.\".format(time_log[-1][1] - time_log[0][1]))\n",
    "#     time_df = pandas.DataFrame(time_log)\n",
    "#     time_df['step_times'] = time_df[1].diff()\n",
    "\n",
    "#     pprint(time_df[[0, 'step_times']])\n",
    "    \n",
    "    \n",
    "# df, time_log = timed_make_Nd_dataframe(test_pseudodata)\n",
    "# process_time_log(time_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudodata = Pseudodata(conserve_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pseudodata.list_available_distributions(detailed_list=True)[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pseudodata.add_feature()\n",
    "pseudodata.add_feature(size=100)\n",
    "pseudodata.add_feature(size=5, distribution='poisson')\n",
    "pseudodata.add_feature(distribution='normal(loc=5, size=100)')\n",
    "\n",
    "my_generator = (number / 37.0 for number in range(38))\n",
    "def my_function():\n",
    "    global my_generator\n",
    "    try:\n",
    "        return next(my_generator)\n",
    "    except:\n",
    "        my_generator = (number / 37.0 for number in range(38))\n",
    "        return next(my_generator)\n",
    "pseudodata.add_feature(distribution='my_function', size=50)\n",
    "\n",
    "pseudodata.show_data_profile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pseudodata.generate_dataframe(with_time_estimate=True)\n",
    "print(\"{0:,} rows, {1} columns.\".format(*dataframe.shape))\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.hist()\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing memory savings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_instance = Pseudodata()\n",
    "\n",
    "for _ in range(4):\n",
    "    regular_instance.add_feature(size=50)\n",
    "\n",
    "%timeit regular_instance.generate_dataframe()\n",
    "regular_instance.generate_dataframe().info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_instance = Pseudodata(conserve_memory=True)\n",
    "\n",
    "for _ in range(4):\n",
    "    reduced_instance.add_feature(size=50)\n",
    "\n",
    "%timeit reduced_instance.generate_dataframe()\n",
    "reduced_instance.generate_dataframe().info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting function strings\n",
    "Make `test` wrapper to contain `try`/`except` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(function):\n",
    "#     def wrapper(*args):\n",
    "#         function(*args)\n",
    "#     return wrapper\n",
    "\n",
    "def test(function):\n",
    "    def wrapper(*args):\n",
    "        try:\n",
    "            print(\"Function result:\", function(*args))\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    return wrapper\n",
    "\n",
    "@test\n",
    "def iscallable(string):\n",
    "    \"\"\"Tests if the string represents a callable function.\"\"\"\n",
    "    return callable(eval(string))\n",
    "\n",
    "@test\n",
    "def iseval(string):\n",
    "    \"\"\"Tests if the string represents an evaluatable object.\"\"\"\n",
    "    return eval(string)\n",
    "\n",
    "# print(iseval(1))\n",
    "# print(iseval(\"1\"))\n",
    "# print(eval(\"1\"))\n",
    "\n",
    "print(iscallable(\"1\"))\n",
    "print(callable(eval(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method chaining\n",
    "\n",
    "Example\n",
    "\n",
    ">`Pseudodata().add_array(100).add_array('binomial(10,0.5)').generate_dataframe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    value = list()\n",
    "    def __init__(self):\n",
    "        self.value = [1]\n",
    "    \n",
    "    def add_one(self, inplace=False):\n",
    "        self.value.append(self.value[-1] + 1)\n",
    "        return self\n",
    "    \n",
    "    def print_value(self):\n",
    "        print(self.value)\n",
    "        \n",
    "b = Test()\n",
    "b.add_one().add_one().add_one().add_one()\n",
    "print(\"b.print_value():\", b.print_value())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated working methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OBSOLECENSE TIMELINE UNKNOWN\n",
    "# def make_dataframe(array_1=None, array_2=None):\n",
    "#     \"\"\"Create a DataFrame from the elementwise product of two iterables.\"\"\"\n",
    "    \n",
    "#     if array_1 is None or array_2 is None:\n",
    "#         return None\n",
    "    \n",
    "#     if array_1.shape[0] < array_2.shape[0]:\n",
    "#         index_array = array_1\n",
    "#         column_array = array_2\n",
    "#     else:\n",
    "#         index_array = array_2\n",
    "#         column_array = array_1\n",
    "\n",
    "#     result_dataframe = pd.DataFrame(index=index_array, columns=column_array)\n",
    "#     for row in result_dataframe.iterrows():\n",
    "#         result_dataframe.loc[row[0]] = row[0] * result_dataframe.columns\n",
    "#     return result_dataframe\n",
    "\n",
    "\n",
    "# # PHASED OUT 4-July-2018\n",
    "# # MADE OBSOLETE BY make_Nd_dataframe\n",
    "# def _make_dataframe(self, array_1=None, array_2=None):\n",
    "#     \"\"\"Create a DataFrame from the elementwise product of two iterables.\"\"\"\n",
    "\n",
    "#     result_dataframe = pd.DataFrame(index=array_1, columns=array_2)\n",
    "\n",
    "#     # Iterative operations are generally faster if row length is greater than the number of columns.\n",
    "#     if result_dataframe.shape[0] < result_dataframe.shape[1]:\n",
    "#         result_dataframe = result_dataframe.T\n",
    "\n",
    "#     result_dataframe = result_dataframe.apply(lambda series: series.index) * result_dataframe.columns\n",
    "\n",
    "#     return result_dataframe\n",
    "\n",
    "# # PHASED OUT 4-July-2018\n",
    "# # PAIRED WITH _make_dataframe\n",
    "# # MADE OBSOLETE BY make_Nd_dataframe\n",
    "# def _invert_dataframe(self, input_dataframe=None):\n",
    "#     \"\"\"Inverts a DataFrame, where the values become the index and the column and row indicies become values.\"\"\"\n",
    "#     if input_dataframe is None:\n",
    "#         return None\n",
    "#     reshaped_dataframe = input_dataframe.stack().reset_index().set_index(0)\n",
    "#     feature_count = reshaped_dataframe.shape[1]\n",
    "#     feature_names = list(range(feature_count))\n",
    "#     reshaped_dataframe.columns = feature_names\n",
    "#     return reshaped_dataframe.sort_index()\n",
    "\n",
    "\n",
    "# # PHASED OUT 20-July-2018\n",
    "# # MADE OBSOLETE BY: SEE COMMENT\n",
    "# def _make_Nd_dataframe(self, arrays):\n",
    "#     \"\"\"Handles N-dimensions.\"\"\"\n",
    "#     grid = numpy.meshgrid(*arrays)\n",
    "#     transposed_grid = numpy.transpose(grid)\n",
    "#     reshaped_grid = numpy.dstack(transposed_grid).reshape(-1, len(arrays))  #Replacing dstack with ravel significantly reduces reshape time.\n",
    "#     dataframe = pandas.DataFrame(reshaped_grid)\n",
    "#     products = dataframe.product(axis=1)\n",
    "#     dataframe.index = products\n",
    "#     dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "#     dataframe = dataframe.sort_index()\n",
    "#     return dataframe\n",
    "\n",
    "\n",
    "# # PHASED OUT 25-July-2018\n",
    "# # MADE OBSOLETE BY: various improvements using Numpy.\n",
    "# def _make_Nd_dataframe(self, arrays):\n",
    "#     \"\"\"Handles N-dimensions.\"\"\"\n",
    "#     grid = numpy.meshgrid(*arrays)\n",
    "#     transposed_grid = numpy.transpose(grid)\n",
    "#     reshaped_grid = transposed_grid.ravel().reshape(-1, len(arrays))\n",
    "#     dataframe = pandas.DataFrame(reshaped_grid) \n",
    "#     products = dataframe.product(axis=1)\n",
    "#     dataframe.index = products\n",
    "#     dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "#     dataframe = dataframe.sort_index()\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
