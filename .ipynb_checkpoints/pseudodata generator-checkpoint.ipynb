{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make function to make list of random values.\n",
    "* Cross-product values to make dataframe.\n",
    "* Make table values equivalent of x-values.\n",
    "* Have observation and feature values be y-values.\n",
    "* Specify \"messiness\" for data: null values, missing values, outliers\n",
    "\n",
    "* Make a function to visualize results for spot-checking.\n",
    "\n",
    "Make into class\n",
    "* Add data arrays with add_dimension(len, distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development notes**\n",
    "\n",
    "1. Create messiness arguments in add_feature:\n",
    "    * Let user specify percent of data that would be NaN.\n",
    "    * Let user specify percent of data that's incorrect type. (e.g., value = str(numerical_value))\n",
    "    * Let user specify percent of outliers.\n",
    "2. Apply messiness arguments to each feature after generating the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#DEBUGGING\n",
    "from pprint import pprint\n",
    "from sys import getsizeof\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pseudodata:\n",
    "    \"\"\"\n",
    "    Anticipated workflow\n",
    "    1. Create Pseudodata instance.\n",
    "    2. Add two or more distribution arrays.\n",
    "    3. Generate an output pseudodata DataFrame from the distribution arrays.\n",
    "    4. Add an additional array to the dataset. Give option to update the pseudodata DataFrame automatically or manually (with method).\n",
    "    5. Output the pseudodata DataFrame as Pandas object. OPTIONAL OTHER FORMATS?\n",
    "    6. VISUALIZE THE DATAFRAME?\n",
    "    \n",
    "    Development plan\n",
    "    * Make function_string formatter, so that most input string are modified to be evaluatable as callable functions. (Including size arguments for Numpy distribution methods.)\n",
    "    * Make data generation lazy. Adding distributions should only automatically update the data_profile dictionary; the user can change the option or run generate_dataframe or update_dataframe.\n",
    "    * Allow add_array to use non-Numpy distribution functions.\n",
    "    * Allow allow arrays to be regenerated so a new sample can be drawn from the same distribution.\n",
    "    * Allow non-numerical data arrays; i.e., text. (MAP STRING VALUES TO NUMBERS, THEN SUBSTITUTE BACK AFTER MAKING DATAFRAME?)\n",
    "    \n",
    "    Ideas to consider\n",
    "    * \"dataframe\" objects are tuples of the DataFrames and snapshots of data_profiles used for generation. I.e., dataframe = (data_profile, pandas.DataFrame).\n",
    "    * Distribution arrays should be stored. Advantage: able to reference values later and reconstruct DataFrame objects. Disadvantage: memory storage.\n",
    "    * DataFrames are best visualized with pairplots, especially for multiple (2+) dimensions.\n",
    "    * Users should have the option to export DataFrames as CSVs and SQL files.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_profile = dict()\n",
    "    features_data = dict()\n",
    "    dataframe = pandas.DataFrame()\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self._check_setup()\n",
    "        self.data_profile = dict()\n",
    "        self.features_data = dict()\n",
    "        self.dataframe = pandas.DataFrame()\n",
    "            \n",
    "    \n",
    "    def __call__(self):\n",
    "        # WHAT DOES THIS DO?\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _check_setup(self):\n",
    "        try:\n",
    "            modules\n",
    "        except:\n",
    "            from sys import modules\n",
    "        \n",
    "        for module in ['pandas', 'numpy']:\n",
    "            if module not in modules:\n",
    "                print(\"{} not imported\".format(module))\n",
    "\n",
    "\n",
    "    def show_data_profile(self):\n",
    "        \"\"\"Displays a description of the arrays in the Pseudodata instance as a DataFrame object.\"\"\"\n",
    "        return pandas.DataFrame(self.data_profile).T\n",
    "\n",
    "    def list_available_distributions(self, detailed_list=False):\n",
    "        \"\"\"Returns a list of the univariate distributions available in numpy.random.\"\"\"\n",
    "        rand_docstring = numpy.random.__doc__\n",
    "        prefiltered_doc_string = rand_docstring.split('variate distributions')[1].split('\\n')\n",
    "        dist_filter = filter(lambda x: 'distribution' in x, prefiltered_doc_string)\n",
    "        dist_list = [element.split(' ')[0] for element in dist_filter]\n",
    "        \n",
    "        if detailed_list == False:\n",
    "            return dist_list\n",
    "        else:\n",
    "            detailed_dist_list = list()\n",
    "            for distribution in dist_list:\n",
    "                dist_docstring = eval(\"numpy.random.{}.__doc__\".format(distribution))\n",
    "                details = dist_docstring.split('\\n')[1].strip()\n",
    "                detailed_dist_list.append(details)\n",
    "            return detailed_dist_list    \n",
    "\n",
    "    def _is_evaluatable(self, input_string):\n",
    "        \"\"\"Tests if a string refers to an object that can be evaluated.\"\"\"\n",
    "        try:\n",
    "            eval(input_string)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _is_callable(self, input_string):\n",
    "        \"\"\"Tests if a string refers to a callable object.\"\"\"\n",
    "        try:\n",
    "            return callable(eval(input_string))\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _add_size(self, input_string, size):\n",
    "        \"\"\"Adds a size argument to distribution function string if applicable.\"\"\"\n",
    "        if 'size' in input_string or size == None:\n",
    "            return input_string\n",
    "        elif '()' in input_string:\n",
    "            return input_string.replace('()', '(size={0})'.format(size))\n",
    "        else:\n",
    "            return input_string.replace(')', ', size={0})'.format(size))\n",
    "\n",
    "    def _format_function_string(self, input_string, size_argument=None):\n",
    "        \"\"\"IN PROGRESS: use \"test\" decorator\n",
    "        Makes any necessary changes to the distribution function string so that it can make feature data.\"\"\"\n",
    "        if input_string.split('(')[0] in self.list_available_distributions():\n",
    "            input_string = 'numpy.random.{0}'.format(input_string)\n",
    "        if self._is_evaluatable(input_string) == False:\n",
    "            return input_string\n",
    "        callable_string_cases = {True: input_string + \"()\", \n",
    "                                 False: input_string}\n",
    "        callable_string = callable_string_cases.get(self._is_callable(input_string))\n",
    "        test_string = self._add_size(callable_string, size_argument)\n",
    "        try:\n",
    "            eval(test_string)\n",
    "            return test_string\n",
    "        except:\n",
    "            return callable_string\n",
    "    \n",
    "    def _make_data_array(self, distribution='normal', list_len=10):\n",
    "        \"\"\"Creates a 1-D Numpy array of specified length using the specified univariate distribution function from Numpy.random.\"\"\"\n",
    "        function_string = self._format_function_string(input_string=distribution, size_argument=list_len)\n",
    "        test_value = eval(function_string)\n",
    "        if isinstance(test_value, type(numpy.array([]))) or isinstance(test_value, list):\n",
    "            result = eval(function_string)\n",
    "        else:\n",
    "            result = [eval(function_string) for _ in range(list_len)]\n",
    "        return numpy.array(result)\n",
    "\n",
    "    def _make_Nd_dataframe(self, arrays):\n",
    "        \"\"\"Handles N-dimensions.\"\"\"\n",
    "        grid = numpy.meshgrid(*arrays)\n",
    "        transposed_grid = numpy.transpose(grid)\n",
    "        reshaped_grid = transposed_grid.ravel().reshape(-1, len(arrays))\n",
    "        dataframe = pandas.DataFrame(reshaped_grid)\n",
    "        products = dataframe.product(axis=1)\n",
    "        dataframe.index = products\n",
    "        dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "        dataframe = dataframe.sort_index()\n",
    "        return dataframe\n",
    "    \n",
    "    \n",
    "    # ADD/REMOVE FEATURES DOES NOT TRACK LAST DELETED INDEX\n",
    "    def add_feature(self, distribution='normal', size=10, remake_dataframe=False):\n",
    "        \"\"\"Adds a data array to the Pseudodata instance. \n",
    "        \n",
    "        Refer to Pseudodata.list_available_distributions() to see available distibution options. Examples:\n",
    "        Pseudodata_instance.add_array()\n",
    "        Pseudodata_instance.add_array(distribution='poisson')\n",
    "        Pseudodata_instance.add_array(distribution='binomial(10, .5, size=10)')\n",
    "        Pseudodata_instance.add_array(distribution='logistic(loc=5.0, scale=2.0)', size=10)\n",
    "        \"\"\"\n",
    "        data_array = self._make_data_array(distribution=distribution, list_len=size)\n",
    "        feature_id = max(self.data_profile.keys(), default=-1) + 1\n",
    "        self.data_profile[feature_id] = {'size': data_array.shape[0], 'distribution': distribution}\n",
    "        self.features_data[feature_id] = data_array\n",
    "        \n",
    "        if remake_dataframe == True:\n",
    "            self._make_Nd_dataframe(self.features_data.values())\n",
    "\n",
    "    def remove_feature(self, feature_index, remake_dataframe=False):\n",
    "        \"\"\"Removes a feature by data profile index.\"\"\"\n",
    "        self.data_profile.pop(feature_index, None)\n",
    "        self.features_data.pop(feature_index, None)\n",
    "        if remake_dataframe == True:\n",
    "            self._make_Nd_dataframe(self.features_data.values())\n",
    "            \n",
    "#     def remove_all_features(self, clear_dataframe=False):\n",
    "#         \"\"\"Deletes all data features, with an option to remove the DataFrame.\"\"\"\n",
    "#         self.data_profile = dict()\n",
    "#         self.features_data = dict()\n",
    "#         if clear_dataframe == True:\n",
    "#             self.dataframe = pandas.DataFrame()\n",
    "            \n",
    "    def generate_dataframe(self, feature_index=None, reduce_memory=False):\n",
    "        \"\"\"IN PROGRESS: apply messiness arguments\n",
    "        Generates a new dataframe from all features in the Pseudodata instance or from a specified list of data profile indicies.\n",
    "        There is an option to reduce memory size of the dataframe by downcasting, which can significantly cut memory by reducing data precision.\"\"\"\n",
    "        if feature_index != None:\n",
    "            for item in feature_index:\n",
    "                if item not in self.data_profile:\n",
    "                    raise KeyError(\"{0} is not in the data profile.\".format(item))\n",
    "            filtered_features_data = [self.features_data.get(key) for key in feature_index]\n",
    "            feature_array = numpy.array(filtered_features_data)\n",
    "            self.dataframe = self._make_Nd_dataframe(feature_array)\n",
    "            # APPLY MESSINESS ARGUMENTS\n",
    "            return self.dataframe\n",
    "        else:\n",
    "            self.dataframe = self._make_Nd_dataframe(self.features_data.values())\n",
    "            # APPLY MESSINESS ARGUMENTS\n",
    "            return self.dataframe\n",
    "\n",
    "    def regenerate_feature(self, feature_index):\n",
    "        \"\"\"Resamples a specific feature according to its data profile index.\"\"\"\n",
    "        feature_details = self.data_profile.get(feature_index)\n",
    "        data_array = self._make_data_array(feature_details['size'], feature_details['distribution'])\n",
    "        self.features_data[feature_index] = data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance time testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 56.557029 seconds.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 40960000 entries, -177.40723688423566 to 175.59672241941314\n",
      "Data columns (total 4 columns):\n",
      "feature_0    float64\n",
      "feature_1    float64\n",
      "feature_2    float64\n",
      "feature_3    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 1.5 GB\n",
      "None\n",
      "DataFrame dimensions: 40,960,000 rows, 4 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>step_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Init array</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make meshgrid</td>\n",
       "      <td>0.950787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transpose grids</td>\n",
       "      <td>1.058415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ravel grid</td>\n",
       "      <td>12.080427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reshape grid</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Recast as DataFrame</td>\n",
       "      <td>0.001974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Calculate product column</td>\n",
       "      <td>19.796492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rename index and columns</td>\n",
       "      <td>0.018905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorting DataFrame</td>\n",
       "      <td>22.649993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0  step_times\n",
       "0                Init array         NaN\n",
       "1             Make meshgrid    0.950787\n",
       "2           Transpose grids    1.058415\n",
       "3                Ravel grid   12.080427\n",
       "4              Reshape grid    0.000036\n",
       "5       Recast as DataFrame    0.001974\n",
       "6  Calculate product column   19.796492\n",
       "7  Rename index and columns    0.018905\n",
       "8         Sorting DataFrame   22.649993"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pseudodata = Pseudodata()\n",
    "\n",
    "new_pseudodata.add_feature(size=80)\n",
    "new_pseudodata.add_feature(size=80)\n",
    "new_pseudodata.add_feature(size=80, distribution='poisson')\n",
    "new_pseudodata.add_feature(distribution='normal(loc=5, size=80)')\n",
    "\n",
    "def timed_make_Nd_dataframe(pseudodata_obj):\n",
    "    \"\"\"Handles N-dimensions.\"\"\"\n",
    "    time_log = []\n",
    "    start_time = time()\n",
    "    \n",
    "    arrays = pseudodata_obj.features_data.values()\n",
    "    time_log.append(('Init array', time()))\n",
    "                    \n",
    "    grid = numpy.meshgrid(*arrays)\n",
    "    time_log.append(('Make meshgrid', time()))\n",
    "    \n",
    "    transposed_grid = numpy.transpose(grid)\n",
    "    time_log.append(('Transpose grids', time()))\n",
    "    \n",
    "    raveled_grid = transposed_grid.ravel()\n",
    "    time_log.append(('Ravel grid', time()))\n",
    "    \n",
    "    reshaped_grid = raveled_grid.reshape(-1, len(arrays))\n",
    "    time_log.append(('Reshape grid', time()))\n",
    "    \n",
    "    dataframe = pandas.DataFrame(reshaped_grid)\n",
    "    time_log.append(('Recast as DataFrame', time()))\n",
    "    \n",
    "    products = dataframe.product(axis=1)\n",
    "    time_log.append(('Calculate product column', time()))\n",
    "    \n",
    "    dataframe.index = products\n",
    "    dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "    time_log.append(('Rename index and columns', time()))\n",
    "    \n",
    "    dataframe = dataframe.sort_index()\n",
    "    time_log.append(('Sorting DataFrame', time()))\n",
    "    return dataframe, time_log\n",
    "\n",
    "df, time_log = timed_make_Nd_dataframe(new_pseudodata)\n",
    "\n",
    "print(\"Runtime: {0:.6f} seconds.\".format(time_log[-1][1] - time_log[0][1]))\n",
    "pprint(df.info())\n",
    "print(\"DataFrame dimensions: {0:,} rows, {1} columns.\".format(*df.shape))\n",
    "\n",
    "time_df = pandas.DataFrame(time_log)\n",
    "time_df['step_times'] = time_df[1].diff()\n",
    "\n",
    "time_df[[0, 'step_times']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decreasing memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562.50 MB\n",
      "937.50 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_0</th>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1</th>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_2</th>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_3</th>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            before    after\n",
       "feature_0  float64  float32\n",
       "feature_1  float64  float32\n",
       "feature_2  float64  float32\n",
       "feature_3  float64  float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pandas.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "\n",
    "gl_float = df.select_dtypes(include=['float'])\n",
    "converted_float = gl_float.apply(pandas.to_numeric,downcast='float')\n",
    "\n",
    "print(mem_usage(gl_float))\n",
    "print(mem_usage(converted_float))\n",
    "\n",
    "compare_floats = pandas.concat([gl_float.dtypes,converted_float.dtypes],axis=1)\n",
    "compare_floats.columns = ['before','after']\n",
    "compare_floats#.apply(pandas.Series.value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index       -2.349178e-10\n",
       "feature_0    1.919696e-09\n",
       "feature_1   -3.539494e-09\n",
       "feature_2    0.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DOWNCAST ARRAY, TO SPEED UP DataFrame-MAKING PROCESS?\n",
    "# OR DOWNCAST DATAFRAME, TO HAVE BETTER PRECISION?\n",
    "\n",
    "original_data = list(new_pseudodata.features_data.values())[0:3]\n",
    "reduced_data = [item.astype('float32', casting='same_kind') for item in original_data]\n",
    "difference_df = new_pseudodata._make_Nd_dataframe(reduced_data).reset_index() - new_pseudodata._make_Nd_dataframe(original_data).reset_index()\n",
    "difference_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'distribution': 'normal', 'size': 20},\n",
      " 1: {'distribution': 'normal', 'size': 20},\n",
      " 2: {'distribution': 'normal', 'size': 20},\n",
      " 3: {'distribution': 'normal', 'size': 20},\n",
      " 4: {'distribution': 'normal', 'size': 20},\n",
      " 5: {'distribution': 'normal', 'size': 20}}\n",
      "18.2 s ± 905 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "12.6 s ± 635 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# stack_test_pd = Pseudodata()\n",
    "\n",
    "# DIMENSIONS = 5\n",
    "# for _ in range(DIMENSIONS):\n",
    "#     stack_test_pd.add_feature(size=20)\n",
    "\n",
    "# pprint(stack_test_pd.data_profile)\n",
    "    \n",
    "# def make_grid(pseudodata_obj):\n",
    "#     \"\"\"Handles N-dimensions.\"\"\"\n",
    "#     time_log = []\n",
    "#     start_time = time()\n",
    "    \n",
    "#     arrays = pseudodata_obj.features_data.values()\n",
    "#     time_log.append(('Init array', time()))\n",
    "                    \n",
    "#     grid = numpy.meshgrid(*arrays)\n",
    "#     time_log.append(('Make meshgrid', time()))\n",
    "    \n",
    "#     transposed_grid = numpy.transpose(grid)\n",
    "#     time_log.append(('Transpose grids', time()))\n",
    "\n",
    "#     return transposed_grid\n",
    "\n",
    "# test_grid = make_grid(stack_test_pd)\n",
    "\n",
    "# def dataframe_by_dstack(input_grid):\n",
    "#     numpy.dstack(test_grid).reshape(-1, len(stack_test_pd.data_profile))\n",
    "\n",
    "# def dataframe_by_ravel(input_grid):\n",
    "#     input_grid.ravel().reshape(-1, len(stack_test_pd.data_profile))  # First make 1xN array. Reshaping is significantly faster.\n",
    "    \n",
    "# %timeit dataframe_by_dstack(test_grid)\n",
    "# %timeit dataframe_by_ravel(test_grid)\n",
    "\n",
    "# a_df = pandas.DataFrame(numpy.dstack(test_grid).reshape(-1, len(stack_test_pd.data_profile))).sort_values(list(range(DIMENSIONS))).reset_index(drop=True)\n",
    "# b_df = pandas.DataFrame(test_grid.ravel().reshape(-1, len(stack_test_pd.data_profile))).sort_values(list(range(DIMENSIONS))).reset_index(drop=True)\n",
    "\n",
    "# print(\"Both methods yield equivalent results:\")\n",
    "# pprint(((b_df - a_df) == 0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudodata = Pseudodata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pseudodata.list_available_distributions(detailed_list=True)[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pseudodata.add_feature()\n",
    "pseudodata.add_feature(size=100)\n",
    "pseudodata.add_feature(size=5, distribution='poisson')\n",
    "pseudodata.add_feature(distribution='normal(loc=5, size=100)')\n",
    "\n",
    "my_generator = (number / 37.0 for number in range(38))\n",
    "def my_function():\n",
    "    global my_generator\n",
    "    try:\n",
    "        return next(my_generator)\n",
    "    except:\n",
    "        my_generator = (number / 37.0 for number in range(38))\n",
    "        return next(my_generator)\n",
    "pseudodata.add_feature(distribution='my_function', size=50)\n",
    "\n",
    "pseudodata.show_data_profile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pseudodata.generate_dataframe()\n",
    "print(\"{0:,} rows, {1} columns.\".format(*test_df.shape))\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.hist()\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seaborn.pairplot(test_df, markers='+')\n",
    "# plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String formatting cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ci sono 10 mele.\n",
      "Ci sono dieci mele\n"
     ]
    }
   ],
   "source": [
    "my_var = 10\n",
    "italian_dict = {10: \"dieci\"}\n",
    "\n",
    "print(f\"Ci sono {my_var} mele.\")\n",
    "print(\"Ci sono {0} mele\".format(italian_dict[my_var]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting function strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def test(function):\n",
    "#     def wrapper(*args):\n",
    "#         function(*args)\n",
    "#     return wrapper\n",
    "\n",
    "def test(function):\n",
    "    def wrapper(*args):\n",
    "        try:\n",
    "            print(\"Function result:\", function(*args))\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    return wrapper\n",
    "\n",
    "@test\n",
    "def iscallable(string):\n",
    "    \"\"\"Tests if the string represents a callable function.\"\"\"\n",
    "    return callable(eval(string))\n",
    "\n",
    "@test\n",
    "def iseval(string):\n",
    "    \"\"\"Tests if the string represents an evaluatable object.\"\"\"\n",
    "    return eval(string)\n",
    "\n",
    "# print(iseval(1))\n",
    "# print(iseval(\"1\"))\n",
    "# print(eval(\"1\"))\n",
    "\n",
    "print(iscallable(\"1\"))\n",
    "print(callable(eval(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method chaining\n",
    "\n",
    "Example\n",
    "\n",
    ">`Pseudodata().add_array(100).add_array('binomial(10,0.5)').generate_dataframe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Test:\n",
    "    value = list()\n",
    "    def __init__(self):\n",
    "        self.value = [1]\n",
    "    \n",
    "    def add_one(self, inplace=False):\n",
    "        self.value.append(self.value[-1] + 1)\n",
    "        return self\n",
    "    \n",
    "    def print_value(self):\n",
    "        print(self.value)\n",
    "        \n",
    "b = Test()\n",
    "b.add_one().add_one().add_one().add_one()\n",
    "print(\"b.print_value():\", b.print_value())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated working methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # OBSOLECENSE TIMELINE UNKNOWN\n",
    "# def make_dataframe(array_1=None, array_2=None):\n",
    "#     \"\"\"Create a DataFrame from the elementwise product of two iterables.\"\"\"\n",
    "    \n",
    "#     if array_1 is None or array_2 is None:\n",
    "#         return None\n",
    "    \n",
    "#     if array_1.shape[0] < array_2.shape[0]:\n",
    "#         index_array = array_1\n",
    "#         column_array = array_2\n",
    "#     else:\n",
    "#         index_array = array_2\n",
    "#         column_array = array_1\n",
    "\n",
    "#     result_dataframe = pd.DataFrame(index=index_array, columns=column_array)\n",
    "#     for row in result_dataframe.iterrows():\n",
    "#         result_dataframe.loc[row[0]] = row[0] * result_dataframe.columns\n",
    "#     return result_dataframe\n",
    "\n",
    "\n",
    "# # PHASED OUT 4-July-2018\n",
    "# # MADE OBSOLETE BY make_Nd_dataframe\n",
    "# def _make_dataframe(self, array_1=None, array_2=None):\n",
    "#     \"\"\"Create a DataFrame from the elementwise product of two iterables.\"\"\"\n",
    "\n",
    "#     result_dataframe = pd.DataFrame(index=array_1, columns=array_2)\n",
    "\n",
    "#     # Iterative operations are generally faster if row length is greater than the number of columns.\n",
    "#     if result_dataframe.shape[0] < result_dataframe.shape[1]:\n",
    "#         result_dataframe = result_dataframe.T\n",
    "\n",
    "#     result_dataframe = result_dataframe.apply(lambda series: series.index) * result_dataframe.columns\n",
    "\n",
    "#     return result_dataframe\n",
    "\n",
    "# # PHASED OUT 4-July-2018\n",
    "# # PAIRED WITH _make_dataframe\n",
    "# # MADE OBSOLETE BY make_Nd_dataframe\n",
    "# def _invert_dataframe(self, input_dataframe=None):\n",
    "#     \"\"\"Inverts a DataFrame, where the values become the index and the column and row indicies become values.\"\"\"\n",
    "#     if input_dataframe is None:\n",
    "#         return None\n",
    "#     reshaped_dataframe = input_dataframe.stack().reset_index().set_index(0)\n",
    "#     feature_count = reshaped_dataframe.shape[1]\n",
    "#     feature_names = list(range(feature_count))\n",
    "#     reshaped_dataframe.columns = feature_names\n",
    "#     return reshaped_dataframe.sort_index()\n",
    "\n",
    "\n",
    "# # PHASED OUT 20-July-2018\n",
    "# # MADE OBSOLETE BY: SEE COMMENT\n",
    "# def _make_Nd_dataframe(self, arrays):\n",
    "#     \"\"\"Handles N-dimensions.\"\"\"\n",
    "#     grid = numpy.meshgrid(*arrays)\n",
    "#     transposed_grid = numpy.transpose(grid)\n",
    "#     reshaped_grid = numpy.dstack(transposed_grid).reshape(-1, len(arrays))  #Replacing dstack with ravel significantly reduces reshape time.\n",
    "#     dataframe = pandas.DataFrame(reshaped_grid)\n",
    "#     products = dataframe.product(axis=1)\n",
    "#     dataframe.index = products\n",
    "#     dataframe.columns = [\"feature_{0}\".format(column) for column in dataframe.columns]\n",
    "#     dataframe = dataframe.sort_index()\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
